# ğŸ—ï¸ ARQUITETURA BACKEND + FRONTEND + BANCO DE DADOS

## ğŸ“‹ VisÃ£o Geral

### Stack TecnolÃ³gica Proposta

**Backend (API REST)**
- **Framework**: FastAPI (Python)
- **Banco de Dados**: PostgreSQL (produÃ§Ã£o) / SQLite (desenvolvimento)
- **ORM**: SQLAlchemy
- **Migrations**: Alembic
- **Cache**: Redis (opcional, para cache de consultas)
- **Task Queue**: Celery + Redis (para sincronizaÃ§Ã£o em background)

**Frontend (SPA - Single Page Application)**
- **Framework**: React.js com TypeScript
- **UI Library**: Material-UI (MUI) ou Ant Design
- **State Management**: React Query (cache + sincronizaÃ§Ã£o automÃ¡tica)
- **Charts**: Recharts ou Chart.js
- **Build Tool**: Vite

**Infraestrutura**
- **ContainerizaÃ§Ã£o**: Docker + Docker Compose
- **Deploy Backend**: Railway / Render / Fly.io (free tier)
- **Deploy Frontend**: Vercel / Netlify (free tier)
- **CI/CD**: GitHub Actions

---

## ğŸ—„ï¸ Modelagem do Banco de Dados

### Tabelas Principais

```sql
-- 1. EMPRESAS
CREATE TABLE empresas (
    id SERIAL PRIMARY KEY,
    codigo VARCHAR(50) UNIQUE NOT NULL,
    nome VARCHAR(255) NOT NULL,
    cnpj VARCHAR(18),
    regime_tributario VARCHAR(50),
    ativa BOOLEAN DEFAULT true,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- 2. PROCESSOS
CREATE TABLE processos (
    id SERIAL PRIMARY KEY,
    proc_id INTEGER UNIQUE NOT NULL,  -- ID da API AcessÃ³rias
    empresa_id INTEGER REFERENCES empresas(id),
    nome VARCHAR(255) NOT NULL,
    competencia VARCHAR(7) NOT NULL,  -- '10/2025'
    status VARCHAR(50),  -- 'EM_ANDAMENTO', 'CONCLUIDO', 'PENDENTE'
    porcentagem_conclusao DECIMAL(5,2),
    total_passos INTEGER,
    passos_concluidos INTEGER,
    dias_corridos INTEGER,
    data_inicio TIMESTAMP,
    data_conclusao TIMESTAMP,
    regime_tributario VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_competencia (competencia),
    INDEX idx_status (status),
    INDEX idx_empresa (empresa_id)
);

-- 3. PASSOS (Steps)
CREATE TABLE passos (
    id SERIAL PRIMARY KEY,
    passo_id INTEGER NOT NULL,  -- ID da API
    processo_id INTEGER REFERENCES processos(id) ON DELETE CASCADE,
    ordem INTEGER,
    nome VARCHAR(255) NOT NULL,
    descricao TEXT,
    concluido BOOLEAN DEFAULT false,
    responsavel VARCHAR(100),
    data_conclusao TIMESTAMP,
    observacoes TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_processo (processo_id),
    INDEX idx_concluido (concluido)
);

-- 4. DESDOBRAMENTOS
CREATE TABLE desdobramentos (
    id SERIAL PRIMARY KEY,
    desdobramento_id INTEGER,
    passo_id INTEGER REFERENCES passos(id) ON DELETE CASCADE,
    processo_id INTEGER REFERENCES processos(id) ON DELETE CASCADE,
    pergunta TEXT NOT NULL,
    resposta TEXT,
    alternativas JSONB,  -- Array de opÃ§Ãµes
    tipo VARCHAR(50),  -- 'BINARIO', 'MULTIPLA_ESCOLHA'
    ordem INTEGER,
    respondido BOOLEAN DEFAULT false,
    data_resposta TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_processo (processo_id),
    INDEX idx_respondido (respondido)
);

-- 5. OBRIGACOES (DAS, EFD REINF, etc)
CREATE TABLE obrigacoes (
    id SERIAL PRIMARY KEY,
    processo_id INTEGER REFERENCES processos(id) ON DELETE CASCADE,
    tipo VARCHAR(100) NOT NULL,  -- 'DAS', 'EFD_REINF', 'DIFAL', etc
    descricao TEXT,
    status VARCHAR(50),  -- 'PENDENTE', 'ENTREGUE', 'DISPENSADO'
    data_vencimento DATE,
    data_entrega TIMESTAMP,
    valor DECIMAL(15,2),
    codigo_recibo VARCHAR(100),
    arquivo_url TEXT,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    
    INDEX idx_tipo (tipo),
    INDEX idx_status (status),
    INDEX idx_vencimento (data_vencimento)
);

-- 6. SINCRONIZACOES (Log de atualizaÃ§Ãµes)
CREATE TABLE sincronizacoes (
    id SERIAL PRIMARY KEY,
    tipo VARCHAR(50) NOT NULL,  -- 'FULL', 'INCREMENTAL', 'MANUAL'
    competencia VARCHAR(7),
    total_processos INTEGER,
    processos_novos INTEGER,
    processos_atualizados INTEGER,
    status VARCHAR(50),  -- 'INICIADA', 'CONCLUIDA', 'ERRO'
    mensagem_erro TEXT,
    tempo_execucao INTEGER,  -- segundos
    iniciada_em TIMESTAMP DEFAULT NOW(),
    concluida_em TIMESTAMP,
    
    INDEX idx_tipo (tipo),
    INDEX idx_status (status),
    INDEX idx_competencia (competencia)
);

-- 7. USUARIOS (para autenticaÃ§Ã£o futura)
CREATE TABLE usuarios (
    id SERIAL PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    nome VARCHAR(255) NOT NULL,
    senha_hash VARCHAR(255) NOT NULL,
    role VARCHAR(50) DEFAULT 'USER',  -- 'ADMIN', 'MANAGER', 'USER'
    ativo BOOLEAN DEFAULT true,
    ultimo_acesso TIMESTAMP,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- 8. METRICAS_CACHE (para dashboards rÃ¡pidos)
CREATE TABLE metricas_cache (
    id SERIAL PRIMARY KEY,
    competencia VARCHAR(7) NOT NULL,
    regime_tributario VARCHAR(50),
    metrica_tipo VARCHAR(100) NOT NULL,  -- 'TOTAL_PROCESSOS', 'TAXA_CONCLUSAO', etc
    valor JSONB NOT NULL,  -- Armazena objeto com valores
    calculada_em TIMESTAMP DEFAULT NOW(),
    
    UNIQUE(competencia, regime_tributario, metrica_tipo),
    INDEX idx_competencia (competencia),
    INDEX idx_tipo (metrica_tipo)
);
```

---

## ğŸ”„ EstratÃ©gia de SincronizaÃ§Ã£o Otimizada

### 1. **SincronizaÃ§Ã£o Inicial (FULL SYNC)**
```python
# Primeira vez - busca tudo
- Buscar todos os 211 processos da API
- Inserir no banco de dados
- Marcar Ãºltima sincronizaÃ§Ã£o
- Calcular mÃ©tricas iniciais
```

### 2. **SincronizaÃ§Ã£o Incremental (SMART SYNC)**
```python
# EstratÃ©gia otimizada:

1. VERIFICAÃ‡ÃƒO RÃPIDA (a cada 15 min)
   - Buscar apenas METADATA dos processos (sem passos)
   - Comparar hash/timestamp com banco local
   - Identificar processos modificados
   
2. ATUALIZAÃ‡ÃƒO SELETIVA
   - Buscar APENAS processos que mudaram
   - Atualizar registros especÃ­ficos
   - Evitar sobrecarga da API
   
3. LÃ“GICA DE DETECÃ‡ÃƒO DE MUDANÃ‡AS:
   - Campo 'updated_at' na API
   - Comparar porcentagem de conclusÃ£o
   - Verificar total de passos concluÃ­dos
   
4. CACHE INTELIGENTE
   - MÃ©tricas calculadas ficam em cache (tabela metricas_cache)
   - Recalcula apenas quando hÃ¡ mudanÃ§as
   - Dashboard lÃª do cache (sub-segundo)
```

### 3. **SincronizaÃ§Ã£o em Background (CELERY TASKS)**
```python
# Tasks assÃ­ncronas:

@celery.task
def sync_processos_incremental():
    """Roda a cada 15 minutos"""
    1. Buscar lista de processos (lightweight)
    2. Comparar com banco
    3. Atualizar apenas os modificados
    4. Invalidar cache de mÃ©tricas
    
@celery.task
def sync_full_competencia(competencia):
    """SincronizaÃ§Ã£o completa manual"""
    1. Buscar todos os processos
    2. Truncar dados antigos (opcional)
    3. Reinserir tudo
    4. Recalcular mÃ©tricas

@celery.task
def calcular_metricas_diarias():
    """Roda 1x por dia Ã s 00:00"""
    1. Calcular estatÃ­sticas agregadas
    2. Atualizar tabela metricas_cache
    3. Gerar relatÃ³rios automÃ¡ticos
```

---

## ğŸš€ Arquitetura da API (FastAPI)

### Estrutura de DiretÃ³rios

```
backend/
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py                    # Ponto de entrada FastAPI
â”‚   â”œâ”€â”€ config.py                  # ConfiguraÃ§Ãµes (env vars)
â”‚   â”œâ”€â”€ database.py                # ConexÃ£o DB + SessionLocal
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                    # SQLAlchemy Models
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ empresa.py
â”‚   â”‚   â”œâ”€â”€ processo.py
â”‚   â”‚   â”œâ”€â”€ passo.py
â”‚   â”‚   â”œâ”€â”€ desdobramento.py
â”‚   â”‚   â”œâ”€â”€ obrigacao.py
â”‚   â”‚   â””â”€â”€ sincronizacao.py
â”‚   â”‚
â”‚   â”œâ”€â”€ schemas/                   # Pydantic Schemas (validaÃ§Ã£o)
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ processo.py
â”‚   â”‚   â”œâ”€â”€ dashboard.py
â”‚   â”‚   â””â”€â”€ sync.py
â”‚   â”‚
â”‚   â”œâ”€â”€ crud/                      # CRUD operations
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ processo.py
â”‚   â”‚   â”œâ”€â”€ empresa.py
â”‚   â”‚   â””â”€â”€ metricas.py
â”‚   â”‚
â”‚   â”œâ”€â”€ api/                       # Endpoints
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ v1/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ processos.py       # GET /processos, /processos/{id}
â”‚   â”‚   â”‚   â”œâ”€â”€ dashboard.py       # GET /dashboard/metricas
â”‚   â”‚   â”‚   â”œâ”€â”€ sync.py            # POST /sync/manual
â”‚   â”‚   â”‚   â””â”€â”€ empresas.py        # GET /empresas
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                  # LÃ³gica de negÃ³cio
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ acessorias_sync.py     # SincronizaÃ§Ã£o com API
â”‚   â”‚   â”œâ”€â”€ metricas_service.py    # CÃ¡lculo de mÃ©tricas
â”‚   â”‚   â””â”€â”€ cache_service.py       # GestÃ£o de cache
â”‚   â”‚
â”‚   â”œâ”€â”€ tasks/                     # Celery tasks
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ sync_tasks.py
â”‚   â”‚   â””â”€â”€ metricas_tasks.py
â”‚   â”‚
â”‚   â””â”€â”€ utils/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ rate_limiter.py
â”‚       â””â”€â”€ logger.py
â”‚
â”œâ”€â”€ alembic/                       # Migrations
â”‚   â”œâ”€â”€ versions/
â”‚   â””â”€â”€ env.py
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_api.py
â”‚   â””â”€â”€ test_sync.py
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ .env.example
```

### Endpoints da API

```
# PROCESSOS
GET    /api/v1/processos                    # Lista todos (paginado)
GET    /api/v1/processos/{id}               # Detalhes de 1 processo
GET    /api/v1/processos/competencia/{comp} # Filtra por competÃªncia
GET    /api/v1/processos/empresa/{emp_id}   # Processos de 1 empresa
GET    /api/v1/processos/{id}/passos        # Passos do processo
GET    /api/v1/processos/{id}/desdobramentos # Desdobramentos

# DASHBOARD
GET    /api/v1/dashboard/metricas           # MÃ©tricas gerais (cached)
GET    /api/v1/dashboard/regimes            # EstatÃ­sticas por regime
GET    /api/v1/dashboard/timeline           # HistÃ³rico temporal

# SINCRONIZAÃ‡ÃƒO
POST   /api/v1/sync/manual                  # Trigger sync manual
GET    /api/v1/sync/status                  # Status da Ãºltima sync
GET    /api/v1/sync/history                 # HistÃ³rico de syncs

# EMPRESAS
GET    /api/v1/empresas                     # Lista empresas
GET    /api/v1/empresas/{id}/processos      # Processos da empresa

# OBRIGAÃ‡Ã•ES
GET    /api/v1/obrigacoes                   # Lista obrigaÃ§Ãµes
GET    /api/v1/obrigacoes/vencimento/{data} # Por vencimento
```

---

## ğŸ¨ Arquitetura do Frontend (React)

### Estrutura de DiretÃ³rios

```
frontend/
â”œâ”€â”€ public/
â”‚   â””â”€â”€ index.html
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.tsx                   # Entry point
â”‚   â”œâ”€â”€ App.tsx                    # Root component
â”‚   â”‚
â”‚   â”œâ”€â”€ components/                # Componentes reutilizÃ¡veis
â”‚   â”‚   â”œâ”€â”€ Layout/
â”‚   â”‚   â”‚   â”œâ”€â”€ Header.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ Sidebar.tsx
â”‚   â”‚   â”‚   â””â”€â”€ Footer.tsx
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Dashboard/
â”‚   â”‚   â”‚   â”œâ”€â”€ StatCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ RegimeChart.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ TimelineChart.tsx
â”‚   â”‚   â”‚   â””â”€â”€ MetricasGerais.tsx
â”‚   â”‚   â”‚
â”‚   â”‚   â”œâ”€â”€ Processos/
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessosList.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessoCard.tsx
â”‚   â”‚   â”‚   â”œâ”€â”€ ProcessoDetail.tsx
â”‚   â”‚   â”‚   â””â”€â”€ PassosList.tsx
â”‚   â”‚   â”‚
â”‚   â”‚   â””â”€â”€ Common/
â”‚   â”‚       â”œâ”€â”€ LoadingSpinner.tsx
â”‚   â”‚       â”œâ”€â”€ ErrorBoundary.tsx
â”‚   â”‚       â””â”€â”€ Pagination.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ pages/                     # PÃ¡ginas/Rotas
â”‚   â”‚   â”œâ”€â”€ DashboardPage.tsx
â”‚   â”‚   â”œâ”€â”€ ProcessosPage.tsx
â”‚   â”‚   â”œâ”€â”€ ProcessoDetailPage.tsx
â”‚   â”‚   â”œâ”€â”€ EmpresasPage.tsx
â”‚   â”‚   â””â”€â”€ SyncPage.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                  # API calls
â”‚   â”‚   â”œâ”€â”€ api.ts                 # Axios config
â”‚   â”‚   â”œâ”€â”€ processosService.ts
â”‚   â”‚   â”œâ”€â”€ dashboardService.ts
â”‚   â”‚   â””â”€â”€ syncService.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ hooks/                     # Custom React hooks
â”‚   â”‚   â”œâ”€â”€ useProcessos.ts
â”‚   â”‚   â”œâ”€â”€ useDashboard.ts
â”‚   â”‚   â””â”€â”€ useSync.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ contexts/                  # React Context
â”‚   â”‚   â”œâ”€â”€ AuthContext.tsx
â”‚   â”‚   â””â”€â”€ ThemeContext.tsx
â”‚   â”‚
â”‚   â”œâ”€â”€ types/                     # TypeScript types
â”‚   â”‚   â”œâ”€â”€ processo.ts
â”‚   â”‚   â”œâ”€â”€ dashboard.ts
â”‚   â”‚   â””â”€â”€ empresa.ts
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â”œâ”€â”€ formatters.ts
â”‚   â”‚   â””â”€â”€ constants.ts
â”‚   â”‚
â”‚   â””â”€â”€ styles/
â”‚       â””â”€â”€ theme.ts               # MUI theme customization
â”‚
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â”œâ”€â”€ vite.config.ts
â””â”€â”€ Dockerfile
```

### Exemplo de Component (StatCard.tsx)

```typescript
import { Card, CardContent, Typography, Box } from '@mui/material';
import TrendingUpIcon from '@mui/icons-material/TrendingUp';

interface StatCardProps {
  label: string;
  value: number | string;
  icon: React.ReactNode;
  color: string;
  trend?: number;
}

export const StatCard: React.FC<StatCardProps> = ({
  label,
  value,
  icon,
  color,
  trend
}) => {
  return (
    <Card sx={{ 
      background: `linear-gradient(135deg, ${color}22 0%, ${color}44 100%)`,
      transition: 'transform 0.3s',
      '&:hover': { transform: 'translateY(-5px)' }
    }}>
      <CardContent>
        <Box display="flex" justifyContent="space-between">
          <Box>
            <Typography variant="caption" color="textSecondary">
              {label}
            </Typography>
            <Typography variant="h3" fontWeight="bold">
              {value}
            </Typography>
          </Box>
          <Box sx={{ color }}>{icon}</Box>
        </Box>
        {trend && (
          <Box display="flex" alignItems="center" mt={1}>
            <TrendingUpIcon fontSize="small" />
            <Typography variant="caption" ml={0.5}>
              +{trend}% vs. mÃªs anterior
            </Typography>
          </Box>
        )}
      </CardContent>
    </Card>
  );
};
```

---

## ğŸ”„ LÃ³gica de SincronizaÃ§Ã£o Otimizada

### Algoritmo de DetecÃ§Ã£o de MudanÃ§as

```python
# backend/app/services/acessorias_sync.py

from datetime import datetime
from typing import List, Dict
import hashlib

class AcessoriasSyncService:
    
    async def sync_incremental(self, competencia: str) -> Dict:
        """
        SincronizaÃ§Ã£o incremental otimizada
        """
        start_time = datetime.now()
        
        # 1. Buscar lista resumida da API (LEVE)
        api_processos = await self.fetch_processos_metadata(competencia)
        
        # 2. Buscar processos existentes no banco
        db_processos = await self.get_processos_from_db(competencia)
        
        # 3. Comparar e identificar mudanÃ§as
        novos = []
        atualizados = []
        
        for api_proc in api_processos:
            db_proc = db_processos.get(api_proc['proc_id'])
            
            if not db_proc:
                # Processo novo
                novos.append(api_proc['proc_id'])
            elif self.has_changes(api_proc, db_proc):
                # Processo modificado
                atualizados.append(api_proc['proc_id'])
        
        # 4. Buscar DETALHES apenas dos processos que mudaram
        if novos or atualizados:
            processos_ids = novos + atualizados
            
            # Busca em paralelo (max 5 simultÃ¢neos para nÃ£o sobrecarregar)
            processos_detalhados = await self.fetch_processos_batch(
                processos_ids,
                batch_size=5
            )
            
            # 5. Atualizar banco de dados
            await self.upsert_processos(processos_detalhados)
            
            # 6. Invalidar cache de mÃ©tricas
            await self.invalidate_metrics_cache(competencia)
        
        # 7. Registrar sincronizaÃ§Ã£o
        tempo_execucao = (datetime.now() - start_time).seconds
        await self.log_sync(
            tipo='INCREMENTAL',
            competencia=competencia,
            processos_novos=len(novos),
            processos_atualizados=len(atualizados),
            tempo_execucao=tempo_execucao
        )
        
        return {
            'status': 'CONCLUIDA',
            'novos': len(novos),
            'atualizados': len(atualizados),
            'tempo_execucao': tempo_execucao
        }
    
    def has_changes(self, api_proc: Dict, db_proc: Dict) -> bool:
        """
        Detecta se processo mudou comparando campos-chave
        """
        # EstratÃ©gia 1: Comparar hash
        api_hash = self.calculate_hash(api_proc)
        db_hash = db_proc.get('hash_snapshot')
        
        if api_hash != db_hash:
            return True
        
        # EstratÃ©gia 2: Comparar campos especÃ­ficos
        if api_proc.get('porcentagem_conclusao') != db_proc.get('porcentagem_conclusao'):
            return True
        
        if api_proc.get('passos_concluidos') != db_proc.get('passos_concluidos'):
            return True
        
        # EstratÃ©gia 3: Verificar timestamp (se API fornecer)
        if api_proc.get('updated_at'):
            api_updated = datetime.fromisoformat(api_proc['updated_at'])
            db_updated = db_proc.get('updated_at')
            if api_updated > db_updated:
                return True
        
        return False
    
    def calculate_hash(self, processo: Dict) -> str:
        """
        Gera hash do processo para detectar mudanÃ§as
        """
        # Campos relevantes para comparaÃ§Ã£o
        relevant_data = {
            'status': processo.get('status'),
            'porcentagem': processo.get('porcentagem_conclusao'),
            'passos_concluidos': processo.get('passos_concluidos'),
            'total_passos': processo.get('total_passos')
        }
        
        data_str = str(sorted(relevant_data.items()))
        return hashlib.md5(data_str.encode()).hexdigest()
    
    async def fetch_processos_batch(
        self,
        processos_ids: List[int],
        batch_size: int = 5
    ) -> List[Dict]:
        """
        Busca processos em lotes paralelos
        """
        import asyncio
        
        processos = []
        
        # Divide em lotes
        for i in range(0, len(processos_ids), batch_size):
            batch = processos_ids[i:i + batch_size]
            
            # Busca paralela
            tasks = [
                self.fetch_processo_detail(proc_id)
                for proc_id in batch
            ]
            batch_results = await asyncio.gather(*tasks)
            processos.extend(batch_results)
            
            # Rate limiting - pausa entre batches
            await asyncio.sleep(1)
        
        return processos
```

---

## ğŸ“Š Cache de MÃ©tricas

```python
# backend/app/services/metricas_service.py

class MetricasService:
    
    async def get_dashboard_metricas(self, competencia: str) -> Dict:
        """
        Busca mÃ©tricas do cache ou calcula se necessÃ¡rio
        """
        # 1. Tentar buscar do cache
        cached = await self.get_from_cache(competencia, 'METRICAS_GERAIS')
        
        if cached and self.is_cache_valid(cached):
            return cached['valor']
        
        # 2. Cache invÃ¡lido - recalcular
        metricas = await self.calcular_metricas(competencia)
        
        # 3. Salvar no cache
        await self.save_to_cache(
            competencia=competencia,
            metrica_tipo='METRICAS_GERAIS',
            valor=metricas
        )
        
        return metricas
    
    async def calcular_metricas(self, competencia: str) -> Dict:
        """
        Calcula todas as mÃ©tricas do zero
        """
        # Queries otimizadas no banco
        query = """
        SELECT 
            COUNT(*) as total_processos,
            SUM(CASE WHEN status = 'CONCLUIDO' THEN 1 ELSE 0 END) as concluidos,
            AVG(porcentagem_conclusao) as media_conclusao,
            AVG(dias_corridos) as media_dias,
            regime_tributario,
            COUNT(DISTINCT empresa_id) as total_empresas
        FROM processos
        WHERE competencia = :competencia
        GROUP BY regime_tributario
        """
        
        result = await self.db.execute(query, {'competencia': competencia})
        
        # Processar resultados
        metricas = {
            'total_processos': 0,
            'total_empresas': 0,
            'taxa_conclusao': 0,
            'regimes': {}
        }
        
        for row in result:
            metricas['total_processos'] += row.total_processos
            metricas['total_empresas'] += row.total_empresas
            
            metricas['regimes'][row.regime_tributario] = {
                'total': row.total_processos,
                'concluidos': row.concluidos,
                'taxa_conclusao': (row.concluidos / row.total_processos * 100) if row.total_processos > 0 else 0,
                'media_dias': round(row.media_dias, 1)
            }
        
        return metricas
```

---

## ğŸ³ Docker Compose

```yaml
# docker-compose.yml

version: '3.8'

services:
  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: acessorias_db
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: senha_segura
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      - "5432:5432"
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U admin"]
      interval: 10s
      timeout: 5s
      retries: 5

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  backend:
    build: ./backend
    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload
    volumes:
      - ./backend:/app
    ports:
      - "8000:8000"
    environment:
      DATABASE_URL: postgresql://admin:senha_segura@postgres:5432/acessorias_db
      REDIS_URL: redis://redis:6379/0
      ACESSORIAS_API_TOKEN: ${ACESSORIAS_API_TOKEN}
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started

  celery_worker:
    build: ./backend
    command: celery -A app.tasks worker --loglevel=info
    volumes:
      - ./backend:/app
    environment:
      DATABASE_URL: postgresql://admin:senha_segura@postgres:5432/acessorias_db
      REDIS_URL: redis://redis:6379/0
      ACESSORIAS_API_TOKEN: ${ACESSORIAS_API_TOKEN}
    depends_on:
      - postgres
      - redis

  celery_beat:
    build: ./backend
    command: celery -A app.tasks beat --loglevel=info
    volumes:
      - ./backend:/app
    environment:
      DATABASE_URL: postgresql://admin:senha_segura@postgres:5432/acessorias_db
      REDIS_URL: redis://redis:6379/0
    depends_on:
      - postgres
      - redis

  frontend:
    build: ./frontend
    command: npm run dev
    volumes:
      - ./frontend:/app
      - /app/node_modules
    ports:
      - "5173:5173"
    environment:
      VITE_API_URL: http://localhost:8000/api/v1

volumes:
  postgres_data:
  redis_data:
```

---

## ğŸ“ˆ Estimativa de Performance

### SincronizaÃ§Ã£o Inicial (211 processos)
- **Tempo**: ~3-5 minutos
- **API Calls**: 212 (1 lista + 211 detalhes)
- **InserÃ§Ãµes DB**: ~2.000 registros (processos + passos + desdobramentos)

### SincronizaÃ§Ã£o Incremental (tÃ­pica)
- **Tempo**: ~10-30 segundos
- **API Calls**: 1-15 (apenas processos modificados)
- **Updates DB**: ~50-200 registros

### Dashboard (com cache)
- **Tempo de resposta**: < 100ms
- **Leitura do cache**: ~10ms
- **Sem cache**: ~500ms (recalcula tudo)

---

## ğŸ¯ PrÃ³ximos Passos ImplementaÃ§Ã£o

### Fase 1 - Backend Core (3 dias)
1. âœ… Criar estrutura FastAPI
2. âœ… Modelar banco de dados (SQLAlchemy)
3. âœ… Implementar migrations (Alembic)
4. âœ… Criar endpoints bÃ¡sicos
5. âœ… Implementar serviÃ§o de sincronizaÃ§Ã£o

### Fase 2 - SincronizaÃ§Ã£o Inteligente (2 dias)
1. âœ… LÃ³gica de detecÃ§Ã£o de mudanÃ§as
2. âœ… Sistema de cache de mÃ©tricas
3. âœ… Celery tasks para sync em background
4. âœ… Rate limiting e retries

### Fase 3 - Frontend React (3 dias)
1. âœ… Setup Vite + React + TypeScript
2. âœ… Componentes de dashboard
3. âœ… IntegraÃ§Ã£o com API (React Query)
4. âœ… Charts e visualizaÃ§Ãµes

### Fase 4 - Deploy (1 dia)
1. âœ… Docker Compose local
2. âœ… Deploy backend (Railway)
3. âœ… Deploy frontend (Vercel)
4. âœ… CI/CD com GitHub Actions

### Fase 5 - WhatsApp Integration (2 dias)
1. âœ… Webhook do WhatsApp
2. âœ… Comandos de consulta
3. âœ… NotificaÃ§Ãµes automÃ¡ticas

---

## ğŸ’° Custo Estimado

- **PostgreSQL**: Railway (FREE atÃ© 500MB) ou Supabase (FREE atÃ© 500MB)
- **Backend**: Railway (FREE com $5 crÃ©dito/mÃªs)
- **Frontend**: Vercel (FREE ilimitado para hobby)
- **Redis**: Railway (FREE atÃ© 25MB)
- **Total**: **R$ 0/mÃªs** (tier free) ou **~R$ 25/mÃªs** (produÃ§Ã£o)

---

**Quer que eu comece a implementar? Posso criar:**
1. Backend completo com FastAPI + PostgreSQL
2. Frontend React com dashboard interativo
3. Sistema de sincronizaÃ§Ã£o inteligente
4. Docker Compose para rodar tudo local

**Qual parte vocÃª quer que eu desenvolva primeiro?**
